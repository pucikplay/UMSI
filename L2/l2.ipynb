{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "77d98a4e-9418-4397-8b7a-8088621543bc",
      "metadata": {},
      "source": [
       "# Lista 2\n",
       "\n",
       "## Uczenie maszynowe i sztuczna inteligencja\n",
       "\n",
       "* [Naiwny klasyfikator bayesowski](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) oraz [Naiwny wielomianowy klasyfikator bayesowski](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes)\n",
       "* [Tokenizacja](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)\n",
       "* [Multizbiór słów](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
       "* [N-gram](https://en.wikipedia.org/wiki/N-gram), [Bigram](https://en.wikipedia.org/wiki/Bigram), [Trigram](https://en.wikipedia.org/wiki/Trigram)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "53fd6799-47e0-46c6-b8e3-64a0cb60587c",
      "metadata": {},
      "source": [
       "## Wprowadzenie \n",
       "\n",
       "Spamowanie jest jednym z najprostszych ataków w przesyłaniu wiadomości e-mail. Użytkownicy często otrzymują irytujące wiadomości spamowe oraz złośliwe wiadomości phishingowe, subskrybując różne strony internetowe, produkty, usługi, katalogi, biuletyny informacyjne oraz inne rodzaje komunikacji elektronicznej. W niektórych przypadkach, spamowe wiadomości e-mail są generowane przez wirusy lub konie trojańskie rozsyłane masowo.\n",
       "\n",
       "Istnieje wiele rozwiązań do filtrowania spamu, takich jak techniki filtrowania na czarnej i białej liście, podejścia oparte na drzewach decyzyjnych, podejścia oparte na adresach e-mail oraz metody oparte na uczeniu maszynowym. Większość z nich opiera się głównie na analizie tekstu zawartości e-maila. W rezultacie rośnie zapotrzebowanie na skuteczne filtry antyspamowe, które automatycznie identyfikują i usuwają wiadomości spamowe lub ostrzegają użytkowników przed możliwymi wiadomościami spamowymi. Jednak spamerzy zawsze badają luki istniejących technik filtrowania spamu i wprowadzają nowy projekt do rozprzestrzeniania spamu w szerokim zakresie np. atak tokenizacji czasami wprowadza w błąd filtry antyspamowe, dodając dodatkowe spacje. Dlatego też treści e-maili muszą być strukturalizowane. Ponadto, pomimo posiadania najwyższej dokładności w wykrywaniu spamu za pomocą uczenia maszynowego, fałszywe pozytywy (False Positive, FP) stanowią problem z powodu jednorazowego wykrywania zagrożeń e-mailowych. Aby zaradzić problemom z fałszywymi pozytywami oraz zmianom w różnych projektach ataków, z tekstu usuwane są słowa kluczowe oraz inne niepożądane informacje przed dalszą analizą. Po wstępnym przetwarzaniu, te teksty przechodzą przez liczne metody ekstrakcji cech, takie jak word2vec, word n-gram, character n-gram oraz kombinacje n-gramów o zmiennych długościach. Różne techniki uczenia maszynowego, takie jak support vector machine (SVM), decision tree (DT), logistic regression (LR) oraz multinomial naıve bayes (MNB), są stosowany aby dokonać klasyfikacji e-maili.\n",
       "\n",
       "Na tej liste skoncentrujemy się tylko na metodzie naiwnego klasyfikatora bayesowskiego przedstawionego na wykładzie wraz z wersją [wielomianową](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes).\n",
       "\n",
       "#### **Uwaga**\n",
       "\n",
       "**Wszystkie implementacje klasyfikatorów należy napisać samemu. Na tej liście nie korzystamy z implementacji klasyfikatorów istniejących w popularnych bibliotekach.**"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a550496e-1cac-4071-935a-6f92eec8afd7",
      "metadata": {},
      "source": [
       "\n",
       "# Klasyfikatory Naiwnego Bayesa (NB)\n",
       "\n",
       "W naszych eksperymentach po wstępnym przetworzeniu każda wiadomość jest ostatecznie reprezentowana jako wektor $\\mathbf{x}=(x_1, \\ldots , x_m)$, gdzie $x_1, \\ldots , x_m$ są wartościami atrybutów $X_1, \\ldots , X_m$ , a każdy atrybut dostarcza informacje o określonym tokenie wiadomości. W najprostszym przypadku wszystkie atrybuty są wartościami boolowskimi: $X_i = 1$, jeśli wiadomość zawiera dany token; w przeciwnym razie, $X_i = 0$. Alternatywnie, ich wartości mogą być częstotliwościami tokenów (TF), pokazującymi, ile razy odpowiadający token występuje w wiadomości. Atrybuty z wartościami TF przenoszą więcej informacji niż atrybuty boolowskie.\n",
       "\n",
       "Z twierdzenia Bayesa wynika, że prawdopodobieństwo, że wiadomość o wektorze $\\mathbf{x} = (x_1, \\ldots, x_m)$ należy do kategorii $c$, wynosi: \n",
       "\n",
       "$$\n",
       "p(c | \\mathbf{x}) = \\frac{p(c) \\cdot p(\\mathbf{x} | c)}{p(\\mathbf{x})}\n",
       "$$\n",
       "\n",
       "Ponieważ mianownik nie zależy od kategorii, klasyfikator NB klasyfikuje każdą wiadomość do kategorii, która maksymalizuje $p(c) \\cdot p(\\mathbf{x} | c)$. W przypadku filtrowania spamu oznacza to klasyfikowanie wiadomości jako spamu, gdy: \n",
       "\n",
       "$$\n",
       "\\frac{p(c_s) \\cdot p(\\mathbf{x} | c_s)}{p(c_s) \\cdot p(\\mathbf{x} | c_s) + p(c_h) \\cdot p(\\mathbf{x} | c_h)} > T\n",
       "$$\n",
       "\n",
       "gdzie $T = 0.5$, a $c_h$ i $c_s$ oznaczają kategorie ham i spam. Zmieniając $T$, można zdecydować się na więcej prawdziwych negatywów (poprawnie sklasyfikowane wiadomości ham) kosztem mniej prawdziwych pozytywów (poprawnie sklasyfikowane wiadomości spam), lub odwrotnie. Prawdopodobieństwa a priori $p(c)$ są zwykle szacowane przez podzielenie liczby treningowych wiadomości kategorii $c$ przez łączną liczbę treningowych wiadomości. Prawdopodobieństwa $p(\\mathbf{x} | c)$ są szacowane w różny sposób w każdej wersji NB - patrz wykład.\n",
       "\n",
       "# Naiwny klasyfikator bayesowski wielomianowy (MNB)\n",
       "\n",
       "Klasyfikator [wielomianowy](https://en.wikipedia.org/wiki/Multinomial_distribution) bayesowski z atrybutami TF traktuje każdą wiadomość $d$ jako [multizbiór]((https://en.wikipedia.org/wiki/Bag-of-words_model)) tokenów, zawierający każdy token $t_i$ tyle razy, ile występuje w $d$. Dlatego $d$ można przedstawić jako $\\mathbf{x} = (x_1, ..., x_m)$, gdzie każde $x_i$ to teraz liczba wystąpień $t_i$ w $d$. Ponadto, każda wiadomość $d$ z kategorii $c$ jest postrzegana jako wynik niezależnego wyboru $|d|$ tokenów z $F=\\{t_1,\\ldots,t_m\\}$ z powtórzeniami, z prawdopodobieństwem $p(t_i | c)$ dla każdego $t_i$. Wówczas $p(\\mathbf{x} | c)$ jest rozkładem wielomianowym:\n",
       "\n",
       "$$\n",
       "p(\\mathbf{x} \\mid c) = p(|d|) \\cdot |d|! \\cdot \\prod_{i=1}^{d} \\frac{p(t_i \\mid c)^{x_i}}{x_i !}\n",
       "$$\n",
       "\n",
       "gdzie zakładamy, że $|d|$ nie zależy od kategorii $c$. Jest to dodatkowe uproszczające założenie, które jest bardziej dyskusyjne w filtrowaniu spamu. Na przykład, prawdopodobieństwo otrzymania bardzo długiej wiadomości spamowej wydaje się mniejsze niż prawdopodobieństwo otrzymania równie długiej wiadomości ham. Kryterium klasyfikacji wiadomości jako spamu staje się:\n",
       "\n",
       "$$\n",
       "\\frac{p(c_s) \\cdot \\prod_{i=1}^{m} p(t_i \\mid c_s)^{x_i}}{p(c_s)\\cdot\\prod_{i=1}^{m} p(t_i \\mid c_s)^{x_i} + p(c_h)\\cdot\\prod_{i=1}^{m} p(t_i \\mid c_h)^{x_i}}  > T\n",
       "$$\n",
       "\n",
       "gdzie każde $p(t_i | c)$ jest szacowane jako:\n",
       "\n",
       "$$\n",
       "p(t \\mid c) = \\frac{\\alpha + N_{t,c}}{\\alpha \\cdot m + N_c}\n",
       "$$\n",
       "gdzie $N_{t,c}$ to liczba wystąpień tokena $t$ w treningowych wiadomościach kategorii $c$, podczas gdy $N_c = \\sum_{i=1}^{m} N_{t_i,c}$ to łączna liczba wiadomości treningowych kategorii $c$. W praktyce dodaje się jeszcze parametr $\\alpha$ który reprezentuje wygładzenie (smoothing) i rozwiązuje problem zerowego prawdopodobieństwa, patrz [http://www.paulgraham.com/spam.html](http://www.paulgraham.com/spam.html) (np. $\\alpha=1$).\n",
       "\n",
       "\n",
       "### Przykładowe dane wielomianowe\n",
       "\n",
       "Zatem każda wiadomość $d$  składa się z różnych tokenów $t_i$, a każde z tych $t_i$ należy do słownika $\\mathcal{V}$. Jeśli $\\mathcal{V}$ zawiera np. $8$ tokenów, $t_1,t_2,...,t_8$, a wiadomość to: $t_1 t_2 t_2 t_6 t_3 t_2 t_8$, reprezentacja tej wiadomości będzie następująca:\n",
       "\n",
       "| |$t_1$|$t_2$|$t_3$|$t_4$|$t_5$|$t_6$|$t_7$|$t_8$|\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "|$\\mathbf{x}$| 1|3 |1 | 0| 0|1 | 0|1 |\n",
       "\n",
       "Po dodaniu kilku innych losowych wiadomości, zbiór danych wygląda tak:\n",
       "\n",
       "|$t_1$|$t_2$|$t_3$|$t_4$|$t_5$|$t_6$|$t_7$|$t_8$|$c$|\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1|3 |1 | 0| 0|1 | 0|1 | spam|\n",
       "| 1|0 |0 | 0| 1|1 | 1|3 | ham|\n",
       "| 0|0 |0 | 0| 0|2 | 1|2 | spam|\n",
       "\n",
       "Przyjmując klasy ($1$-spam,$0$-ham) mamy $c = [1,0,1]$. Teraz, porównując z równaniem powyżej,\n",
       "\n",
       "- $N_{t_i,c}$ to liczba wystąpień cechy $t_i$ w każdej unikalnej klasie $c$. Na przykład, dla $c=1$, $N_{t_1,c}=1, N_{t_6,c}=3$\n",
       "- $N_c$ to całkowita liczba wystąpień wszystkich cech w każdej unikalnej klasie $c$. Na przykład, dla $c=1$, $N_c=12$\n",
       "- $m=8$ to całkowita liczba cech\n",
       "- $\\alpha=1$ jest znany jako parametr wygładzania. Jest on potrzebny do problemu zerowego prawdopodobieństwa (patrz [http://www.paulgraham.com/spam.html](http://www.paulgraham.com/spam.html))\n",
       "\n",
       "# Niedomiar zmiennoprzecinkowy (floating point underflow)\n",
       "\n",
       "Aby uniknąć problemu niedomiaru zmiennoprzecinkowego, mnożenie zbioru małych prawdopodobieństw, czyli po prostu iloczyn stanie się zbyt mały, aby go reprezentować i zostanie zastąpiony przez 0. Zamiast obliczać\n",
       "$$\n",
       "P(c) \\prod_{i=1}^m P(t_i | c)\n",
       "$$\n",
       "co może spowodować niedomiar, rozważmy obliczenie logarytmu tego wyrażenia,\n",
       "$$\n",
       "\\log\\left(P(c) \\prod_{i=1}^m P(t_i | c)\\right)\n",
       "$$\n",
       "co równoważnie można zapisać jako\n",
       "$$\n",
       "\\log(P(c))+ \\sum_{i=1}^m \\log(P(t_i | c))\n",
       "$$\n",
       "Następnie zauważ, że jeśli\n",
       "$$\n",
       "\\log(P(c_s))+ \\sum_{i=1}^m \\log(P(t_i | c_s)) > \\log(P(c_h))+ \\sum_{i=1}^m \\log(P(t_i | c_h))\n",
       "$$\n",
       "wtedy, ponieważ $\\log(x) > \\log(y)$ iff $x > y$, to\n",
       "$$\n",
       "P(c_s) \\prod_{i=1}^m P(t_i | c_s) > P(c_h) \\prod_{i=1}^m P(t_i | c_h)\n",
       "$$\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "b7ff1e5a-8599-412a-9494-45699a65a2fb",
      "metadata": {},
      "source": [
       "## Zadanie 1 (10pt)\n",
       "\n",
       "### Klasyfikator oparty na algorytmie NB\n",
       "\n",
       "#### Cel:\n",
       "Zbudować prosty klasyfikator spamu oparty na NB, który będzie w stanie wykryć i odfiltrować niechciane wiadomości e-mail.\n",
       "\n",
       "#### Opis:\n",
       "1. Zbierz zbiór danych zawierający etykiety (spam/nie-spam) oraz treść wiadomości e-mail np. [Enron-Spam](http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html) lub [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) lub [E-mail Spam](https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv) lub ...\n",
       "2. Przygotuj dane poprzez tokenizację słów i usuń zbędne znaki interpunkcyjne.\n",
       "3. Zaimplementuj NB, który będzie w stanie klasyfikować wiadomości jako spam lub nie-spam na podstawie występujących słów.\n",
       "4. Podziel dane na zbiór treningowy i testowy (np. 70% do treningu, 30% do testu).\n",
       "5. Wytrenuj klasyfikator NB na danych treningowych.\n",
       "6. Przetestuj klasyfikator na danych testowych i oceniaj jego skuteczność przy użyciu metryk: [precision i recall](https://en.wikipedia.org/wiki/Precision_and_recall), [f1-score](https://en.wikipedia.org/wiki/F-score) oraz [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision).\n",
       "7. Dokonaj analizy wyników i przedstaw wnioski.\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db03424-f98e-4b03-8cb3-cbb1cb5d4935",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "81a933bb-c803-4e52-a562-e2a47944fb5f",
      "metadata": {},
      "source": [
       "## Zadanie 2 (15pt)\n",
       "\n",
       "### Klasyfikator oparty na n-gramach MNB\n",
       "\n",
       "#### Cel:\n",
       "Zbudować klasyfikator spamu, wykorzystując n-gramy w połączeniu MNB, aby poprawić skuteczność klasyficji wiadomości e-mail.\n",
       "\n",
       "#### Opis:\n",
       "1. Zbierz zbiór danych zawierający etykiety (spam/nie-spam) oraz treść wiadomości e-mail np. [Enron-Spam](http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html) lub [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) lub [E-mail Spam](https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv) lub ...\n",
       "2. Przygotuj dane poprzez tworzenie n-gramów z treści wiadomości e-mail tzn. unigramy, bigramy, trigramy.\n",
       "3. Zaimplementuj MNB, który będzie w stanie klasyfikować wiadomości jako spam lub nie-spam, wykorzystując n-gramy jako cechy.\n",
       "4. Podziel dane na zbiór treningowy i testowy (np. 70% do treningu, 30% do testu).\n",
       "5. Wytrenuj klasyfikator MNB na danych treningowych, wykorzystując n-gramy jako cechy.\n",
       "6. Przetestuj klasyfikator na danych testowych i oceniaj jego skuteczność przy użyciu metryk: [precision i recall](https://en.wikipedia.org/wiki/Precision_and_recall), [f1-score](https://en.wikipedia.org/wiki/F-score) oraz [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision).\n",
       "7. Dokonaj analizy wyników i porównaj je z wynikami klasyfikatora opartego na słowach.\n",
       "8. Przedstaw wnioski dotyczące skuteczności klasyfikatora opartego na n-gramach oraz wpływu różnych typów n-gramów na skuteczność klasyfikacji.\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2135aa-3e93-4bd9-8d6d-467f14e29f3e",
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }